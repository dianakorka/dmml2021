{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nteract": {
      "version": "0.15.0"
    },
    "colab": {
      "name": "Linear_Regression_Deeper.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataMining_and_MachineLearning/blob/master/week5/Linear_Regression_Deeper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1K6vWbOsh95y"
      },
      "source": [
        "# Deeper understanding about linear regression and over-fitting\n",
        "\n",
        "In this notebook we will take a deeper look into linear regression. We will see why in some cases augmenting the data with polynomial features help to get a more accurate model. Also we will see what does it mean when we say our model over-fits and cannot generalize well on unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fANd0gzh950"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp8tIljah951"
      },
      "source": [
        "# Import the libraries we will be using\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set(style='ticks', palette='Set2')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jAvELkqh957"
      },
      "source": [
        "## Motivational example\n",
        "\n",
        "Imagine we have some noisy observations from a nonlinear function. We're going to approximate that function by fitting a polynomial to the observations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_GS29VIh958"
      },
      "source": [
        "num_samples = 100\n",
        "# Set randomness so that we all get the same answer\n",
        "np.random.seed(42)\n",
        "\n",
        "def true_function(X):\n",
        "    return np.sin(1.5 * np.pi * X)\n",
        "\n",
        "def plot_example(X, Y, functions):\n",
        "    # Get some X's to plot the functions\n",
        "    X_test = pd.DataFrame(np.linspace(0, 1, 100), columns=['X'])\n",
        "    # Plot stuff\n",
        "    for key in functions:\n",
        "        plt.plot(X_test, functions[key](X_test), label=key)\n",
        "    plt.scatter(X, Y, edgecolor='b', s=20, label=\"Samples\")\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    plt.xlim((0, 1))\n",
        "    plt.ylim((-2, 2))\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "# Add X in the range of [0, 1]\n",
        "X = pd.DataFrame(np.sort(np.random.rand(num_samples)), columns=['x1'])\n",
        "# Add some random noise to the observations\n",
        "Y = true_function(X.x1) + np.random.randn(num_samples) * 0.5\n",
        "# Plot stuff\n",
        "functions = {\"True function\": true_function}\n",
        "plt.figure(figsize=(8,5))\n",
        "plot_example(X, Y, functions)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTLcXdkrh96B"
      },
      "source": [
        "Let's assume that we don't know the true function, so we choose to model our noisy observations using linear regression. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaiQCxb4h96C"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Fit linear model\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "# Evaluate model with mean squared error; just as an example\n",
        "mse = mean_squared_error(Y, model.predict(X))\n",
        "# Plot results\n",
        "functions[\"Model\"] = model.predict\n",
        "plt.figure(figsize=(8,5))\n",
        "plot_example(X, Y, functions)\n",
        "#Note how you can customize your plots\n",
        "plt.title(\"Linear Model\\n MSE: %.2f\" % mse)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjoiBX3hh96G"
      },
      "source": [
        "Clearly the linear regression doesn't fit our data super well. Rather than trying a linear regression, let's attempt polynomial regression. How do different degree polynomials fit the data? Recall that a polynomial on a single variable looks like:\n",
        "\n",
        "$$ a_1 + a_2 x + a_3 x^2 + ... $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLUGtpzBh96G"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "def fit_polynomial(X, Y, degree):\n",
        "    # create different powers of X\n",
        "    polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "    linear_regression = LinearRegression()# we did't include bias, so we set fit_intercept as True\n",
        "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features), (\"linear_regression\", linear_regression)])\n",
        "    pipeline.fit(X, Y)\n",
        "    return pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngDYZzBXh96K"
      },
      "source": [
        "def plot_poly(X, Y, degree):\n",
        "    # Fit polynomial model\n",
        "    model = fit_polynomial(X, Y, degree)\n",
        "    # Evaluate model\n",
        "    mse = mean_squared_error(Y, model.predict(X))\n",
        "    # Plot results\n",
        "    functions[\"Model\"] = model.predict\n",
        "    plt.title(\"Degree %d\\n MSE: %.2f\" % (degree, mse))\n",
        "    plot_example(X, Y, functions)\n",
        "    \n",
        "plot_poly(X, Y, degree=2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x3MCPFRh96P"
      },
      "source": [
        "This seems to fit our data better than the purely linear model. What if we use polynomials with higher degrees?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "fxqN5ftIh96P"
      },
      "source": [
        "plt.figure(figsize=(14, 5))\n",
        "# degrees of the polynomial\n",
        "degrees = [1, 4, 15, 30, 50]\n",
        "for i in range(len(degrees)):\n",
        "    ax = plt.subplot(1, len(degrees), i + 1)\n",
        "    plt.setp(ax, xticks=(), yticks=())\n",
        "    plot_poly(X, Y, degrees[i])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UIrvgMBh96U"
      },
      "source": [
        "let's see the effect of increasing polynomial degree through train/test error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLju-Xidh96V"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train , X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n",
        "tr_errors=[]\n",
        "te_errors=[]\n",
        "for d in range(2, 20):\n",
        "    # Fit polynomial model\n",
        "    model = fit_polynomial(X_train, Y_train, d)\n",
        "    # Evaluate model\n",
        "    mse_train = mean_squared_error(Y_train, model.predict(X_train))\n",
        "    mse_test = mean_squared_error(Y_test, model.predict(X_test))\n",
        "    tr_errors.append(mse_train)\n",
        "    te_errors.append(mse_test)\n",
        "    \n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(2,20), tr_errors, label=\"train error\")\n",
        "plt.plot(range(2,20), te_errors, label=\"test error\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbw3SupMeYKg"
      },
      "source": [
        "What we saw above is often reffered to as __bias vs variance trade-off__.\n",
        "\n",
        "__Bias__ refers to the error due to the modelâ€™s simplistic assumptions in fitting the data. A high bias means that the model is unable to capture the patterns in the data and this results in __under-fitting__.\n",
        "\n",
        "__Variance__ refers to the error due to the complex model trying to fit the data. High variance means the model passes through most of the data points and it results in __over-fitting__ the data.\n",
        "\n",
        "If you need more insight on this phenomenon, I would suggest to follow [this blog post](https://towardsdatascience.com/polynomial-regression-bbe8b9d97491)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmbTMJgmh96t"
      },
      "source": [
        "We use a regularized model for the polynomial of degree 10. We will use cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgrDJINoh96t"
      },
      "source": [
        "from sklearn.linear_model import RidgeCV\n",
        "polynomial_features = PolynomialFeatures(degree=10, include_bias=False)\n",
        "X_poly = polynomial_features.fit_transform(X)\n",
        "ridge = RidgeCV()\n",
        "ridge.fit(X_poly, Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80qCMTBJh96w"
      },
      "source": [
        "# best regularizer parameter\n",
        "ridge.alpha_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOoKIS24h96y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}