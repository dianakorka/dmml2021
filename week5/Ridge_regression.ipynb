{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Ridge_regression.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataMining_and_MachineLearning/blob/master/week5/Ridge_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzc978scbU2D"
      },
      "source": [
        "# Linear regression: Standardiztion, Ridge regression, Cross-validation, and more.\n",
        "\n",
        "In this notebook, we are going to tackle the problem of predicting house prices in more details. We will follow the following steps:\n",
        "\n",
        "    1- Preprocessing the data\n",
        "        1.1- One-hot encoding of the categorical columns\n",
        "        1.2- Feature engineering: selecting a subset of columns based on their correlation with the target variable.\n",
        "    2- Splitting the data into training and test sets.\n",
        "    3- Normalization of the data\n",
        "    4- Training a ridge regression model\n",
        "    5- Cross-validation to find the best regularizer hyper-parameter\n",
        "    6- Evaluating the model on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46vJYxBQbU2I"
      },
      "source": [
        "# Useful starting lines\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import collections  as mc\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "sns.set_style(\"white\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVIckeavbU2K"
      },
      "source": [
        "## 0- Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDB4lCvIbU2L"
      },
      "source": [
        "housing_data = pd.read_csv(\"https://raw.githubusercontent.com/michalis0/DataMining_and_MachineLearning/master/week5/data/processed_housing_data.csv\")\n",
        "housing_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyEHfa2IbU2L"
      },
      "source": [
        "housing_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGUf-xWdbU2M"
      },
      "source": [
        "print(housing_data.dtypes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVrHC0IibU2N"
      },
      "source": [
        "## 1- Preprocessing\n",
        "\n",
        "### 1.1- One-hot encoding of the categorical columns\n",
        "\n",
        "We can observe that we have non-numerical columns in our dataset. We have to get rid of them and convert them to numerical values before feeding the data to the model. These columns actuaclly have categorical values, meaning that they only take values from a fixed set of values. For example, let's take a look at the `MSZoning` column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La-haE4gbU2O"
      },
      "source": [
        "housing_data[\"MSZoning\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBhc828EbU2P"
      },
      "source": [
        "We can convert such columns to numerical values using one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH4b2ZacbU2Q"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "ohe = OneHotEncoder()\n",
        "mszoning_ohe = ohe.fit_transform(housing_data[[\"MSZoning\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90BUA68xbU2R"
      },
      "source": [
        "mszoning_ohe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqddNsxzbU2R"
      },
      "source": [
        "# order of the categories\n",
        "ohe.categories_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlni9wyabU2S"
      },
      "source": [
        "# looking at the first rwo of the matrix\n",
        "mszoning_ohe[0].todense()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMEWntoDbU2S"
      },
      "source": [
        "As you can see the one-hot encoder returns an array of `number of datapoints` $\\times$ `number of categoreis`. Each column of this array corresponds to a single category and if it is one, it indicates the categorical value for that data point.\n",
        "\n",
        "However, with this representation we have to convert the resulting array to a dataframe and also name each column of the resulting dataframe according to the categorical value. With the `get_dummies` method from pandas we can actually do the one-hot encoding more easily.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAV2d_YlbU2T"
      },
      "source": [
        "mszoning_ohe = pd.get_dummies(housing_data[[\"MSZoning\"]])\n",
        "mszoning_ohe.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycPivDQnbU2U"
      },
      "source": [
        "so let's extract all the categorical columns and encode them all at once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r9kIvbsbU2U"
      },
      "source": [
        "df_types = pd.DataFrame(housing_data.dtypes).reset_index()\n",
        "string_columns = df_types[df_types[0]=='object']['index']\n",
        "non_string_columns = df_types[df_types[0]!='object']['index']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR4Bx7S6bU2U"
      },
      "source": [
        "print(string_columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo_-MtCnbU2V"
      },
      "source": [
        "dummy_df = pd.get_dummies(housing_data[string_columns])\n",
        "# creating the ultimate dataframe where all categorical columns are one-hot encoded\n",
        "df = pd.concat([dummy_df,housing_data[non_string_columns]], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdiFoA3YbU2V"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jaUEWQObU2Y"
      },
      "source": [
        "df.dtypes.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoh8tH1JbU2Y"
      },
      "source": [
        "### 1.2- Feature engineering\n",
        "\n",
        "Right now there are almost 231 features. In the following we will try to decrease the number of features based on their correlation to the target.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lDajNMmbU2Y"
      },
      "source": [
        "df_corr = abs(df.corr()).sort_values(by='SalePrice', ascending=False)[['SalePrice']]\n",
        "df_corr[df_corr['SalePrice']>0.4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2w_J-n_bU2Z"
      },
      "source": [
        "# let's only keep the columns with a correlation greater than 0.4\n",
        "df_small = df[df_corr[df_corr['SalePrice']>0.4].index.tolist()]\n",
        "df_small.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUuQUt7ybU2Z"
      },
      "source": [
        "Now we decreased the number of features to 24. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VLGRNErbU2Z"
      },
      "source": [
        "### 2- Splitting the data into training and test sets.\n",
        "\n",
        "Next we split the data to training and test sets. The reason we do this is that we want to start the training phase after this step. Any test data that I will introduce during modelling or premodelling, will create a bias in my evaluation metrics. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SRRgWUdbU2Z"
      },
      "source": [
        "df_small.reset_index(inplace=True,drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPUOMM2RbU2Z"
      },
      "source": [
        "X = df_small.drop(columns=['SalePrice'])\n",
        "y = df_small['SalePrice']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG8Iol4ybU2a"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtPdZMohbU2a"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YPEF3GcbU2a"
      },
      "source": [
        "### 3- Normalization of the data\n",
        "\n",
        "Now we can start with normalize the data. This is helpful to bring all the data to the same scale. We will use the sklearn standard scaler which removes the mean from each featrues and divids it by it standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWetyDHObU2a"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Instantiate Standard Scaler.\n",
        "ss = StandardScaler()\n",
        "# fit \n",
        "Z_train = ss.fit_transform(X_train)\n",
        "# transform the df\n",
        "Z_train = pd.DataFrame(ss.transform(X_train), columns=X_train.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2t6sZniwbU2a"
      },
      "source": [
        "Z_train.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ikmht7ZTbU2b"
      },
      "source": [
        "### 4- Training a ridge regression model\n",
        "\n",
        "Since our data is ready, we can start with the modelling phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAZV6viobU2b"
      },
      "source": [
        "from sklearn.linear_model import Ridge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7z_brdjbU2b"
      },
      "source": [
        "First, Let's visualize the regularization hyper-parameter of ridge and its impact on the regression coefficients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foaSXvZQbU2b"
      },
      "source": [
        "def ridge_coefs(X, y, alphas):\n",
        "    \n",
        "    # list of coefficients:\n",
        "    coefs = []\n",
        "    \n",
        "    # initiate the model\n",
        "    ridge_reg = Ridge()\n",
        "    \n",
        "    # iterate through the alphas fed into the function:\n",
        "    for a in alphas:\n",
        "        \n",
        "        # reinitiate with the new alpha:\n",
        "        ridge_reg.set_params(alpha=a)\n",
        "        \n",
        "        # refit the model on the provided X, y\n",
        "        ridge_reg.fit(X, y)\n",
        "        \n",
        "        # print the coefficient list\n",
        "        coefs.append(ridge_reg.coef_)\n",
        "        \n",
        "    return coefs\n",
        "  # this snippet is taken from an online source"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpfkiocLbU2b"
      },
      "source": [
        "# np.logspace gives us points between specified orders of magnitude on a logarithmic scale. It is base 10.\n",
        "r_alphas = np.logspace(0, 5, 200)\n",
        "\n",
        "# Get the coefficients for each alpha for the Ridge, using the function above\n",
        "r_coefs = ridge_coefs(Z_train, y_train, r_alphas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBdCHAmHbU2c"
      },
      "source": [
        "from cycler import cycler\n",
        "\n",
        "def coef_plotter(alphas, coefs, feature_names, to_alpha, regtype='ridge'):\n",
        "    \n",
        "    # Get the full range of alphas before subsetting to keep the plots from \n",
        "    # resetting axes each time. (We use these values to set static axes later).\n",
        "    amin = np.min(alphas)\n",
        "    amax = np.max(alphas)\n",
        "    \n",
        "    # Subset the alphas and coefficients to just the ones below the set limit\n",
        "    # from the interactive widget:\n",
        "    alphas = [a for a in alphas if a <= to_alpha]\n",
        "    coefs = coefs[0:len(alphas)]\n",
        "    \n",
        "    # Get some colors from seaborn:\n",
        "    colors = sns.color_palette(\"husl\", len(coefs[0]))\n",
        "    \n",
        "    # Get the figure and reset the size to be wider:\n",
        "    fig = plt.figure()\n",
        "    fig.set_size_inches(18,5)\n",
        "\n",
        "    # We have two axes this time on our figure. \n",
        "    # The fig.add_subplot adds axes to our figure. The number inside stands for:\n",
        "    #[figure_rows|figure_cols|position_of_current_axes]\n",
        "    ax1 = fig.add_subplot(121)\n",
        "    \n",
        "    # Give it the color cycler:\n",
        "    ax1.set_prop_cycle(cycler('color', colors))\n",
        "    \n",
        "    # Print a vertical line showing our current alpha threshold:\n",
        "    ax1.axvline(to_alpha, lw=2, ls='dashed', c='k', alpha=0.4)\n",
        "    \n",
        "    # Plot the lines of the alphas on x-axis and coefficients on y-axis\n",
        "    ax1.plot(alphas, coefs, lw=2)\n",
        "    \n",
        "    # set labels for axes:\n",
        "    ax1.set_xlabel('alpha', fontsize=20)\n",
        "    ax1.set_ylabel('coefficients', fontsize=20)\n",
        "    \n",
        "    # If this is for the ridge, set this to a log scale on the x-axis:\n",
        "    if regtype == 'ridge':\n",
        "        ax1.set_xscale('log')\n",
        "    \n",
        "    # Enforce the axis limits:\n",
        "    ax1.set_xlim([amin, amax])\n",
        "    \n",
        "    # Put a title on the axis\n",
        "    ax1.set_title(regtype+' coefficients\\n', fontsize=20)\n",
        "    \n",
        "    # Get the ymin and ymax for this axis to enforce it to be the same on the \n",
        "    # second chart:\n",
        "    ymin, ymax = ax1.get_ylim()\n",
        "\n",
        "    # Add our second axes for the barplot in position 2:\n",
        "    ax2 = fig.add_subplot(122)\n",
        "    \n",
        "    # Position the bars according to their index from the feature names variable:\n",
        "    ax2.bar(list(range(1, len(feature_names)+1)), coefs[-1], align='center', color=colors)\n",
        "    ax2.set_xticks(list(range(1, len(feature_names)+1)))\n",
        "    \n",
        "    # Reset the ticks from numbers to acutally be the names:\n",
        "    ax2.set_xticklabels(feature_names, rotation=65, fontsize=12)\n",
        "    \n",
        "    # enforce limits and add titles, labels\n",
        "    ax2.set_ylim([ymin, ymax])\n",
        "    ax2.set_title(regtype+' predictor coefficients\\n', fontsize=20)\n",
        "    ax2.set_xlabel('coefficients', fontsize=20)\n",
        "    ax2.set_ylabel('alpha', fontsize=20)\n",
        "    \n",
        "    plt.show()\n",
        "  # this snippet is taken from an online source"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSy3ZmM6bU2c"
      },
      "source": [
        "from ipywidgets import *\n",
        "from IPython.display import display\n",
        "\n",
        "def ridge_plot_runner(log_of_alpha=0):\n",
        "    coef_plotter(r_alphas, r_coefs, X.columns, 10**log_of_alpha, regtype='ridge')\n",
        "\n",
        "interact(ridge_plot_runner, log_of_alpha=(0.1,5,0.2))\n",
        "print(\"this snippet is taken from an online source\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8hglpHnbU2c"
      },
      "source": [
        "### 5- Cross-validation to find the best regularizer hyper-parameter\n",
        "\n",
        "Thanks to the visualization, we can understand the relationship between the alpha (strength of the regularization coefficient) and coefficients. But how to pick the best alpha? The answer can be found by trial and error. In this case, we will train different models on the training set and pick the model that performs the best. We will do this using cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfzdJPT5bU2c"
      },
      "source": [
        "from sklearn.metrics import r2_score\n",
        "from sklearn.linear_model import RidgeCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNmdfC2AbU2d"
      },
      "source": [
        "# Set up a list of ridge alphas to check.\n",
        "r_alphas = np.logspace(0, 5, 100)\n",
        "# Generates 200 values equally between 0 and 5,\n",
        "# then converts them to alphas between 10^0 and 10^5.\n",
        "\n",
        "# Cross-validate over our list of ridge alphas.\n",
        "ridge_model = RidgeCV(alphas=r_alphas, scoring='r2')\n",
        "\n",
        "# Fit model using best ridge alpha!\n",
        "ridge_model = ridge_model.fit(Z_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkJa-fVCbU2d"
      },
      "source": [
        "# Here is the optimal value of alpha\n",
        "ridge_optimal_alpha = ridge_model.alpha_\n",
        "ridge_optimal_alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kkm1Z0WbU2d"
      },
      "source": [
        "### 6- Evaluating the model on the test data\n",
        "\n",
        "To evaluate the model on the test data, we first need to normalize the test data __using the same normalization that we used to normalize the training data.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0Nwq4O2bU2d"
      },
      "source": [
        "# transform the test data\n",
        "Z_test = pd.DataFrame(ss.transform(X_test), columns=X_test.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VbHN0KEbU2d"
      },
      "source": [
        "# Instantiate the best model.\n",
        "ridge_opt = Ridge(alpha=ridge_optimal_alpha)\n",
        "\n",
        "# Fit model.\n",
        "ridge_opt.fit(Z_train, y_train)\n",
        "\n",
        "# Generate predictions\n",
        "ridge_opt_preds = ridge_opt.predict(Z_test)\n",
        "ridge_opt_preds_train = ridge_opt.predict(Z_train)\n",
        "\n",
        "# Evaluate model.\n",
        "print(\"score on the test data: \", r2_score(y_test, ridge_opt_preds))\n",
        "print(\"score on the training data:\", r2_score(y_train, ridge_opt_preds_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eJ2CQsgbU2d"
      },
      "source": [
        "So the conclusion, the R squared value for the test data was 0.82. This is higher than the score from the training dataset which proves that in a dataset that might highly overfit, we achieved to fit our model to the signal rather than the noise. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfIxltIjbU2e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}