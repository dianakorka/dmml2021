{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.3",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.15.0"
    },
    "colab": {
      "name": "EDA and Data visualization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataMining_and_MachineLearning/blob/master/week3/EDA_and_Data_visualization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiZk0zaywk60"
      },
      "source": [
        "# Data Mining and Machine Learning - Week 3\n",
        "# EDA and Data visualization\n",
        "\n",
        "This notebooks introduces to Exploratory Data Analysis and visualization, mainly using the pandas library.\n",
        "\n",
        "### Table of Contents\n",
        "#### 1. Basic Example: Creating a DataFrame from Scratch\n",
        "#### 2. Restaurant inspection results in NYC\n",
        "* 2.1 Reading a CSV file\n",
        "* 2.2 Descriptive statistics\n",
        "* 2.3 Descriptive Statistics for Numeric Variables\n",
        "* 2.4 Converting Dates\n",
        "* 2.5 Categorical Variables\n",
        "* 2.6 Analyzing the content of the columns\n",
        "* 2.7 Selecting a subset of the columns\n",
        "* 2.8 Selecting rows\n",
        "* 2.9 Pivot Tables\n",
        "* 2.10 (Optional, FYI) Advanced Pivot Tables\n",
        "* 2.11 Exercises\n",
        "\n",
        "#### 3. NYPD Vehicle Collisions\n",
        "* 3.1 Introduction to the dataset\n",
        "* 3.2 Plots, graphs and pivot tables\n",
        "* 3.3 2d histograms, density plots, and contour plots\n",
        "* 3.4 Combining plots\n",
        "* 3.5 Bonus plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt2TiGTLwk62"
      },
      "source": [
        "## Setup and preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UO9Q_uZBwk63"
      },
      "source": [
        "# Render our plots inline\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk4xTsmUwk69"
      },
      "source": [
        "# Import requiered packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyOUJNyGwk7B"
      },
      "source": [
        "We type some code to simply change the visual style of the plots. (The code below is optional and not necessary, and for now you do not need to understand what exactly is happening.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPCtvOSNwk7B"
      },
      "source": [
        "# Modifying the style of the graphs\n",
        "matplotlib.style.use(['seaborn-talk', 'seaborn-ticks', 'seaborn-whitegrid'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RpjEshUwk7E"
      },
      "source": [
        "## 1. Basic Example: Creating a DataFrame from Scratch\n",
        "\n",
        "This is an example of creating a dataframe by passing a list of dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPHBt6p2wk7F"
      },
      "source": [
        "df = pd.DataFrame([ \n",
        "    {\"First Name\": \"Michalis\", \"Last Name\":\" Vlachos\"},\n",
        "    {\"First Name\": \"John\", \"Last Name\":\" Doe\"},\n",
        "    {\"Last Name\":\"Trump\"}\n",
        "])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySsfQQRcwk7M"
      },
      "source": [
        "## 2. Restaurant inspection results in NYC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S72XqgQ4wk7O"
      },
      "source": [
        "### 2.1 Reading a CSV file\n",
        "We can directly load csv file from the url with pandas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "my8IXs96wk7O"
      },
      "source": [
        "url = 'https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv?accessType=DOWNLOAD'\n",
        "restaurants = pd.read_csv(url, \n",
        "                          encoding='utf_8', \n",
        "                          dtype = 'unicode',\n",
        "                          parse_dates = True,\n",
        "                          infer_datetime_format = True,\n",
        "                          low_memory=False)\n",
        "\n",
        "# Let's take a look at the first 5 rows of the dataframe\n",
        "restaurants.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx5nCWJTwk7T"
      },
      "source": [
        "Alternatively, we can first download it and then load it from a local directory in our computer (in case we are not using colab). The `pd.read_csv` method has many options, and you can further read in the [online documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.parsers.read_csv.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZa_rUO9wk7d"
      },
      "source": [
        "### 2.2 Descriptive statistics\n",
        "\n",
        "We can use the method `describe()` to get a quick overview of the data in the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML7hpRWIwk7d"
      },
      "source": [
        "restaurants.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUAHKFrhwk7i"
      },
      "source": [
        "We can also check the data types for each column using `dtypes`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1gvSdQxwk7j"
      },
      "source": [
        "restaurants.dtypes\n",
        "# also try: restaurants.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja1miMrNwk7n"
      },
      "source": [
        "The `object` type is a string. For some of them, we would like to change the data types using the `pd.to_numeric` and `pd.to_datetime` functions. We examine how to convert data types in the next subsection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZOcE6Mawk7o"
      },
      "source": [
        "### 2.3 Descriptive Statistics for Numeric Variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tip1FFSwk7o"
      },
      "source": [
        "#### 2.3.1 Converting Data Types to Numeric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nIG-reRwk7p"
      },
      "source": [
        "The `object` type is a string. When we want to convert an object to numeric, we can use the `pd.to_numeric` function, as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRVUsDWcwk7p"
      },
      "source": [
        "restaurants[[\"SCORE\"]].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NY0_wqNwk7t"
      },
      "source": [
        "# convert score to numeric\n",
        "restaurants[\"SCORE\"] = pd.to_numeric(restaurants[\"SCORE\"])\n",
        "restaurants.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfbetS3Owk7v"
      },
      "source": [
        "#### 2.3.2 Basic descriptive statistics for numeric variables\n",
        "\n",
        "And now that SCORE is a numeric variable, we can get more detailed descriptive statistics for the variable using the `.describe()` command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FhWZr6Lwk7v"
      },
      "source": [
        "# For column names without a space, we can also access directly the column as follows:\n",
        "restaurants.SCORE.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGGgbb6twk71"
      },
      "source": [
        "#### 2.3.3 A first histogram\n",
        "And now that SCORE is a numeric variable, we can examine its distribution by using the `hist` command of Pandas, which creates a histogram. (The histogram is also available as `plot.hist()`, or `plot(kind='hist')`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCjhKEfZwk73"
      },
      "source": [
        "restaurants[\"SCORE\"].hist()\n",
        "# restaurants[\"SCORE\"].plot(kind='hist')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1VQZyK2wk76"
      },
      "source": [
        "By default, the histogram has ~10 bars. We can change the resolution of the histogram using the `bins` attribute. Larger numbers of `bins` allow for higher resolution, but if we increase the number too much, many bins end up having very few, or no data points. For example, experiment with changing the number of bins below, and change the value from 50 to something greater."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c-_QIA7wk76"
      },
      "source": [
        "restaurants[\"SCORE\"].hist(bins=50) # try with e.g. bins=200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7NBTKkhwk79"
      },
      "source": [
        "# A quick exposure to various options of the \"hist\" command\n",
        "restaurants.SCORE.hist(bins=50, # use 50 bars\n",
        "                          range=(0,50), # x-axis from 0 to 50\n",
        "                          density=False,  # show normalized count (density=True), or raw counts (density= False)\n",
        "                          figsize=(15,5), # controls the size of the plot\n",
        "                          alpha=0.8, # make the plot 20% transparent\n",
        "                          color='green' # change color\n",
        "                         )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OvrysBMwk7_"
      },
      "source": [
        "#### 2.3.4 Kernel Density Estimation (KDE)\n",
        "\n",
        "An alternative to histograms is to use the **kernel density**, which estimates a continuous function, instead of the bucketized counts, which tends to be discontinuous and bumpy. We can access this usind the `.plot(kind='kde')` command:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7X1ZClcwk7_"
      },
      "source": [
        "# This plots the density of a numeric attribute\n",
        "# kde = kernel density estimation\n",
        "restaurants.SCORE.plot(\n",
        "    kind='kde', \n",
        "    color='Black', \n",
        "    xlim=(0,50), \n",
        "    figsize=(15,5)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW3liOt3wk8C"
      },
      "source": [
        "### 2.4 Converting Dates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lE7Yujjxwk8C"
      },
      "source": [
        "Now let's convert the dates columns into the appropriate data types. Let's take a look at a few dates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lot8FYyDwk8D"
      },
      "source": [
        "restaurants[\"GRADE DATE\"].sample(10) # 10 random dates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-w5wIqQwk8G"
      },
      "source": [
        "To this end, we first need to understand how to [parse dates using the Python conventions](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior).\n",
        "\n",
        "The relevant entries from the table are:\n",
        "* `%m` Month as a zero-padded decimal number.\n",
        "* `%d` \tDay of the month as a zero-padded decimal number.\n",
        "* `%Y` Year with century as a decimal number.\n",
        "\n",
        "Now, we can specify how to parse the dates. (In principle, we can let Pandas do this automatically, but it is _much_ faster if we specify it ourselves. It is also much less error-prone)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wELAwKOdwk8H"
      },
      "source": [
        "restaurants[\"GRADE DATE\"] = pd.to_datetime(restaurants[\"GRADE DATE\"], format=\"%m/%d/%Y\")\n",
        "restaurants[\"RECORD DATE\"] = pd.to_datetime(restaurants[\"RECORD DATE\"], format=\"%m/%d/%Y\")\n",
        "restaurants[\"INSPECTION DATE\"] = pd.to_datetime(restaurants[\"INSPECTION DATE\"], format=\"%m/%d/%Y\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqF-GmWgwk8J"
      },
      "source": [
        "restaurants.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvbOh4u9wk8N"
      },
      "source": [
        "restaurants[[\"INSPECTION DATE\", \"GRADE DATE\", \"RECORD DATE\"]].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdZ_rH_uwk8Q"
      },
      "source": [
        "#### Dates: exercise\n",
        "\n",
        "* Plot a histogram for `INSPECTION DATE`, `GRADE DATE`, `RECORD DATE`. What do you see?\n",
        "* Try modifying the `bins` parameter. What are the results?\n",
        "* The `range=(start, finish)` command is often useful, when we want to focus on a particular part of the dataset. Try using that for  `INSPECTION DATE` to limit the dates to be between 1/1/2014 and 05/31/2018."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "solution2": "hidden",
        "id": "Ov1pLR51wk8R"
      },
      "source": [
        "# Not very appealing\n",
        "restaurants['INSPECTION DATE'].hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "solution2": "hidden",
        "id": "8mcbag4Ywk8U"
      },
      "source": [
        "restaurants['GRADE DATE'].hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "solution2": "hidden",
        "id": "Po--NIBcwk8Y"
      },
      "source": [
        "restaurants['INSPECTION DATE'].hist(\n",
        "    range = (pd.to_datetime('1-1-2014'),pd.to_datetime('9-30-2018')), # limit the range of dates, ignore the 1/1/1990 faulty valye\n",
        "    bins = 57, # number of months in the range -- computed manually\n",
        "    figsize = (15,5) # resize 15-width, 5-height\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "solution2": "hidden",
        "id": "5yxaPemYwk8c"
      },
      "source": [
        "restaurants['GRADE DATE'].hist(\n",
        "    range = (pd.to_datetime('1-1-2014'),pd.to_datetime('9-30-2018')), # limit the range of dates, ignore the 1/1/1990 faulty valye\n",
        "    bins = 57, # number of months in the range -- computed manually\n",
        "    figsize = (15,5) # resize 15-width, 5-height\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGQEuXrAwk8g"
      },
      "source": [
        "### 2.5 Categorical Variables\n",
        "\n",
        "This is less important, but sometimes we want to specify variables to be \"Categorical\". This is most commonly useful when we have variables that have an implicit order (e.g., the A/B/C grade of the restaurant)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNtbtAHkwk8g"
      },
      "source": [
        "restaurants[\"BORO\"] =  pd.Categorical(restaurants[\"BORO\"], ordered=False)\n",
        "restaurants[\"GRADE\"] =  pd.Categorical(restaurants[\"GRADE\"], categories = ['A', 'B', 'C'], ordered=True)\n",
        "restaurants[\"VIOLATION CODE\"] =  pd.Categorical(restaurants[\"VIOLATION CODE\"], ordered=False)\n",
        "restaurants[\"CRITICAL FLAG\"] =  pd.Categorical(restaurants[\"CRITICAL FLAG\"], ordered=False)\n",
        "restaurants[\"ACTION\"] =  pd.Categorical(restaurants[\"ACTION\"], ordered=False)\n",
        "restaurants[\"CUISINE DESCRIPTION\"] =  pd.Categorical(restaurants[\"CUISINE DESCRIPTION\"], ordered=False)\n",
        "restaurants.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNGvJYWMwk8k"
      },
      "source": [
        "### 2.6 Analyzing the content of the columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hik9O-0Rwk8k"
      },
      "source": [
        "We can also get quick statistics about the common values that appear in each column:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sdaD0Aswk8l"
      },
      "source": [
        "restaurants[\"DBA\"].value_counts()[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn0vAE_swk8n"
      },
      "source": [
        "restaurants[\"CUISINE DESCRIPTION\"].value_counts()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMroOATYwk8q"
      },
      "source": [
        "And we can use the \"plot\" command to plot the resulting histogram (more detail at http://pandas.pydata.org/pandas-docs/stable/visualization.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB5r-v28wk8q"
      },
      "source": [
        "restaurants[\"CUISINE DESCRIPTION\"].value_counts()[:5].plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAATOXFmwk8s"
      },
      "source": [
        "popular = restaurants[\"CUISINE DESCRIPTION\"].value_counts()\n",
        "popular"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGbYsWOIwk8v"
      },
      "source": [
        "Hm, that does not look nice. Let's shorten the name of the cuisine for the _\"Latin (Cuban, Dominican, Puerto Rican, South & Central American)\"_ and replace it with _\"Latin American\"_. We will use the `replace` command in Pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr7_DNMtwk8v"
      },
      "source": [
        "restaurants[\"CUISINE DESCRIPTION\"].replace(\n",
        "    to_replace='Latin (Cuban, Dominican, Puerto Rican, South & Central American)',\n",
        "    value = 'Latin American',\n",
        "    inplace=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrjE8i2rwk8x"
      },
      "source": [
        "restaurants[\"CUISINE DESCRIPTION\"].replace(\n",
        "    to_replace='CafÃ©/Coffee/Tea',\n",
        "    value = 'Cafe/Coffee/Tea',\n",
        "    inplace=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkKNAON2wk82"
      },
      "source": [
        "popular = restaurants[\"CUISINE DESCRIPTION\"].value_counts()\n",
        "popular[:5].plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1DpuojAwk84"
      },
      "source": [
        "Let's do the similar histogram for the violation codes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URdZdjmUwk84"
      },
      "source": [
        "restaurants[\"VIOLATION CODE\"].head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eFYof-owk89"
      },
      "source": [
        "violation_counts = restaurants[\"VIOLATION CODE\"].value_counts();\n",
        "violation_counts[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBUsWB8Rwk9C"
      },
      "source": [
        "violation_counts[0:20].plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00AeH-W1wk9F"
      },
      "source": [
        "#### Plot: Exercise\n",
        "\n",
        "* Create a plot showing the number of inspections that happen across **boroughs**. Use the `BORO` column and the `value_counts()` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "solution2": "hidden",
        "id": "PISzAddHwk9I"
      },
      "source": [
        "restaurants['BORO'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "solution2": "hidden",
        "id": "FaYyV1i8wk9K"
      },
      "source": [
        "# Same as above; works only for attribute names without spaces\n",
        "restaurants.BORO.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "solution2": "hidden",
        "id": "eJDsdmXmwk9O"
      },
      "source": [
        "restaurants['BORO'].value_counts().plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "solution2": "hidden",
        "id": "pfjMWSclwk9R"
      },
      "source": [
        "restaurants['BORO'].value_counts().plot(kind='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhe5c0YJwk9U"
      },
      "source": [
        "### 2.7 Selecting a subset of the columns\n",
        "\n",
        "In a dataframe, we can specify the column(s) that we want to keep, and get back another dataframe with just the subset of the columns that we want to keep."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsKwH7lawk9V"
      },
      "source": [
        "restaurants[ [\"DBA\", \"GRADE\", \"GRADE DATE\"] ].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m65qOxzwk9Y"
      },
      "source": [
        "columns = [\"GRADE DATE\", \"VIOLATION CODE\", \"DBA\", \"SCORE\"]\n",
        "restaurants[ columns ].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUWPDjdSwk9b"
      },
      "source": [
        "### 2.8 Selecting rows\n",
        "\n",
        "To select rows, we can use the following approach, where we generate a list of boolean values, one for each row of the dataframe, and then we use the list to select which of the rows of the dataframe we want to keep."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvMguNeIwk9c"
      },
      "source": [
        "# Create the condition \"has mice\" for code 04L\n",
        "mice = (restaurants[\"VIOLATION CODE\"] == \"04L\")\n",
        "mice.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHE4q-aiwk9e"
      },
      "source": [
        "# Apply the condition to the dataframe \"restaurants\" and store the result \n",
        "# in a dataframe called has_mice\n",
        "has_mice = restaurants[mice]\n",
        "has_mice.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv1yARhFwk9i"
      },
      "source": [
        "# List the most frequent DBA values in the data of restaurants that have mice\n",
        "has_mice[\"DBA\"].value_counts()[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbHNUCidwk9l"
      },
      "source": [
        "has_mice[\"CAMIS\"].value_counts()[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIEC-CfQwk9s"
      },
      "source": [
        "condition = (has_mice[\"CAMIS\"] == '41259444')\n",
        "has_mice[condition].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA5M4O2awk9v"
      },
      "source": [
        "And we can use more complex conditions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hScUFwCbwk9v"
      },
      "source": [
        "mice_Brooklyn =  ( (restaurants[\"VIOLATION CODE\"] == \"04L\") \n",
        "                    & (restaurants[\"BORO\"] == \"Brooklyn\") )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoEAoeBfwk9x"
      },
      "source": [
        "has_mice_brooklyn = restaurants[mice_Brooklyn]\n",
        "has_mice_brooklyn.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rUIz1g5wk92"
      },
      "source": [
        "has_mice_brooklyn[\"DBA\"].value_counts()[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9-1jyK7wk96"
      },
      "source": [
        "### 2.9 Pivot Tables\n",
        "\n",
        "[Pivot tables](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html) is one of the most commonly used exploratory tools, and in Pandas they are extremely flexible. \n",
        "\n",
        "For example, let's try to count the number of restaurants that are inspected every day. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y1GtSBlwk96"
      },
      "source": [
        "pivot = pd.pivot_table(\n",
        "    data = restaurants, \n",
        "    index = 'INSPECTION DATE', # specifies the rows\n",
        "    values = 'CAMIS',  # specifies the content of the cells\n",
        "    aggfunc = 'count' # we ask to count how many different CAMIS values we see\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j-3HZ1-wk99"
      },
      "source": [
        "pivot.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIWryOgewk-B"
      },
      "source": [
        "Now, let's plot this. By default, Pandas considers the \"index\" column to be the x-axis, and plots the rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq1HEsXswk-C"
      },
      "source": [
        "pivot.plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg0RD9BPwk-E"
      },
      "source": [
        "#### 2.9.1 Deleting rows from a table\n",
        "\n",
        "Hm, that '1900-01-01' line is messing things up. Let's delete it, using the `drop` command. Notice a few things:\n",
        "* We use the `pd.to_datetime` command to convert the '1900-01-01' string into a datetime data type.\n",
        "* We use the `axis='index'` parameter means that we delete a **row** with that index value. (The `axis='columns'` means that we delete a column.). Often you will see `axis=0` (equivalent to `axis='index'`) and `axis=1` (equivalent to `axis='columns'`).\n",
        "* The `inplace=True` means that we change directly the dataframe, instead of returning a new dataframe that does not have the deleted value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEO1VtM1wk-E"
      },
      "source": [
        "pivot.drop(pd.to_datetime('1900-01-01'), axis='index', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kRrpCN-wk-I"
      },
      "source": [
        "pivot.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po0qXUTowk-L"
      },
      "source": [
        "And let's plot again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17iqbdqwwk-M"
      },
      "source": [
        "pivot.plot(figsize=(10,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWgHjXJDwk-O"
      },
      "source": [
        "pivot.tail(50).plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzoMFLoSwk-R"
      },
      "source": [
        "#### 2.9.2 Changing date granularity \n",
        "\n",
        "We can also use the [resample](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html) command to change the frequency from one day, to, say, 7 days. Then we can compute, say, the average (`mean()`) for these days, or the total number (`sum()`) of inspections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s9j6HTqwk-R"
      },
      "source": [
        "pivot.resample('1W').mean().tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSdzs1C7wk-V"
      },
      "source": [
        "Now, let's plot this. By default, Pandas considers the \"index\" column to be the x-axis, and plots the rest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAp_0-m1wk-V"
      },
      "source": [
        "# Plot the average number of inspections, over 7-day periods\n",
        "pivot.resample('7D').mean().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdk7pPOuwk-X"
      },
      "source": [
        "# Plot the total number of inspections, over 1-month periods\n",
        "pivot.resample('1M').sum().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0ZSh9jpwk-a"
      },
      "source": [
        "plot = pivot.resample('7D').mean().plot()\n",
        "plot.set_xlabel(\"Date of Inspection\")\n",
        "plot.set_ylabel(\"Average Number of Inspections (7-day average)\")\n",
        "plot.set_title(\"Analysis of Number of Inspections over Time\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9okSlSYHwk-c"
      },
      "source": [
        "#### 2.9.3 Pivot Table with two (or more) variables)\n",
        "\n",
        "We would like to break down the results by **borough**, so we add the `column` parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYkpKohiwk-c"
      },
      "source": [
        "pivot2 = pd.pivot_table(\n",
        "    data = restaurants,\n",
        "    index = 'INSPECTION DATE', \n",
        "    columns = 'BORO', \n",
        "    values = 'CAMIS', \n",
        "    aggfunc = 'count'\n",
        ")\n",
        "pivot2.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uJKKcvnwk-g"
      },
      "source": [
        "###### Deleting rows and columns\n",
        "\n",
        "Now, you will notice that there are a few columns and rows that are just noise. The first row with date *'1900-01-01'* is clearly noise. We can use the `drop` command of Pandas to drop these."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bJwxZgLwk-h"
      },
      "source": [
        "# The axis='index' (or axis=0) means that we delete a row with that index value\n",
        "pivot2 = pivot2.drop(pd.to_datetime('1900-01-01'), axis='index') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MwDTXrQwk-j"
      },
      "source": [
        "pivot2.tail(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOXmFAvtwk-k"
      },
      "source": [
        "# We resample the index, to keep only dates every one month\n",
        "# For that one month period, we compute the average value\n",
        "pivot2.resample('1M').mean().tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMqjuc_DSqVK"
      },
      "source": [
        "If we plot a dataframe then by default the index column in our case the INSPECTION DATE becomes the x-asis and all the columns become separate lines in the plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDFJlW-Wwk-o"
      },
      "source": [
        "pivot2.resample('1M').mean().plot(figsize=(10,5))\n",
        "\n",
        "# Potential Exercise, if we have time: \n",
        "# Drop the last date, which corresponds to an incomplete month"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv3Gmt0fwk-r"
      },
      "source": [
        "### 2.10 (Optional, FYI) Advanced Pivot Tables\n",
        "\n",
        "We can also add multiple attributes in the index and columns. It is also possible to have multiple aggregation functions, and we can even define our own aggregation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg1vNNu4wk-s"
      },
      "source": [
        "# We write a function that returns the \n",
        "# number of unique items in a list x \n",
        "def count_unique(x):\n",
        "    return len(set(x))\n",
        "\n",
        "# We break down by BORO and GRADE, and also calculate \n",
        "# inspections in unique (unique restaurants) \n",
        "# and non-unique entries (effectuvely, violations)\n",
        "pivot_advanced = pd.pivot_table(\n",
        "    data = restaurants,\n",
        "    index = 'GRADE DATE', \n",
        "    columns = ['BORO', 'GRADE'],\n",
        "    values = 'CAMIS', \n",
        "    aggfunc = ['count', count_unique]\n",
        ")\n",
        "\n",
        "# Take the total number of inspections (unique and non-unique)\n",
        "agg = pivot_advanced.resample('1M').sum()\n",
        "\n",
        "# Show the last 5 entries and show the transpose (.T) \n",
        "agg.tail().T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkhVHlADTDoQ"
      },
      "source": [
        "### 2.11 Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vczTft0wk-t"
      },
      "source": [
        "#### Exercise 1 \n",
        "\n",
        "Now let's do the same exercise, but instead of counting the number of inspections, we want to compute the average score assigned by the inspectors. Hint: We will need to change the `values` and the `aggfunc` parameters in the `pivot_table` function above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gbdVFkqwk-t"
      },
      "source": [
        "pivot = pd.pivot_table(\n",
        "    data = restaurants, \n",
        "    index = 'INSPECTION DATE', # specifies the rows\n",
        "    values = 'SCORE',  # specifies the content of the cells\n",
        "    aggfunc = 'mean' # compute the average SCORE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqmrzuMkwk-v"
      },
      "source": [
        "pivot.plot(figsize=(10,4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaW0JEEiwk-y"
      },
      "source": [
        "pivot.resample('1M').mean().tail(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTJDTxoNwk-0"
      },
      "source": [
        "#### Exercise 2\n",
        "\n",
        "We now want to examine if different cuisines have different inspection scores. Compute the average inspection score by cuisine. Use the `sort_values()` command ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html)) to order cuisines by their inspection scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhBbnlQDwk-1"
      },
      "source": [
        "pivot = pd.pivot_table(\n",
        "    data = restaurants, \n",
        "    index = 'INSPECTION DATE', # dates are the rows\n",
        "    columns = 'CUISINE DESCRIPTION', # cuisines are the columns\n",
        "    values = 'SCORE',  # we analyze the SCORE\n",
        "    aggfunc = 'mean' # compute the average SCORE\n",
        ")\n",
        "# Select 3 columns (american, frenchn, chinese), compute the average score for a month\n",
        "# and plot the results\n",
        "pivot[ [\"American\", \"French\", \"Chinese\"] ].resample('1M').mean().plot(figsize=(10,4))\n",
        "plt.ylabel(\"score\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqMOvblbwk-3"
      },
      "source": [
        "## 3. NYPD Vehicle Collisions\n",
        "\n",
        "So farm we interacted with the NYC Restaurant Inspection Data. Now, let's download another dataset, and do some analysis. We will focus on the [NYPD Vehicle Collisions](https://data.cityofnewyork.us/Public-Safety/NYPD-Motor-Vehicle-Collisions/h9gi-nx95/data) data set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOWBR-6jwk-7"
      },
      "source": [
        "### 3.1 Introduction to the dataset\n",
        "Load the accidents dataset from url \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw6aElQ9wk-7"
      },
      "source": [
        "df = pd.read_csv('https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=DOWNLOAD', low_memory=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Eu4nr9P4e3D"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaPumUgYwk-_"
      },
      "source": [
        "Or we can first download it and then load it with pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZyztn6-wk_B"
      },
      "source": [
        "# !curl 'https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=DOWNLOAD' -o data/accidents.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4Ygksw5wk_E"
      },
      "source": [
        "# df = pd.read_csv(\"data/accidents.csv\", low_memory=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61lceS7cwk_H"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXMoU2tgUpST"
      },
      "source": [
        "### 3.2 Plots, graphs and pivot tables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqGlRMz2wk_J"
      },
      "source": [
        "Find out the most common contributing factors to the collisions. \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coGEV_ZTwk_K"
      },
      "source": [
        "df['CONTRIBUTING FACTOR VEHICLE 1'].value_counts().head(11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hC16MAZ3wk_N"
      },
      "source": [
        "Now let's plot a histogram of the above list. Note that we skip the first element."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNWjM8Viwk_N"
      },
      "source": [
        "df['CONTRIBUTING FACTOR VEHICLE 1'].value_counts()[1:11].plot(kind='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS4Wb_pcwk_P"
      },
      "source": [
        "Break down the number of collisions by borough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwfGExyUwk_P"
      },
      "source": [
        "df.BOROUGH.value_counts().plot(kind='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCgpcHewk_Q"
      },
      "source": [
        "Find out how many collisions had 0 person injured, 1 person injured, etc. persons injured in each accident. Use the `value_counts()` approach. You may also find the `.plot(logy=True)` option useful when you create the plot to make the y-axis logarigthmic.\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMSpmACuwk_R"
      },
      "source": [
        "plot = (\n",
        "    df['NUMBER OF PERSONS INJURED'] # take the num of injuries column\n",
        "    .value_counts() # compure the freuquency of each value\n",
        "    .sort_index() # sort the results based on the index value instead of the frequency, \n",
        "                  # which is the default for value_counts\n",
        "    .plot( # and plot the results\n",
        "        kind='line', # we use a line plot because the x-axis is numeric/continuous\n",
        "        marker='o',  # we use a marker to mark where we have data points \n",
        "        logy=True # make the y-axis logarithmic\n",
        "    )\n",
        ")\n",
        "plot.set_xlabel(\"Number of injuries\")\n",
        "plot.set_ylabel(\"Number of collisions\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edXOZ109wk_S"
      },
      "source": [
        "Break down the accidents by borough and contributing factor. Use the `pivot_table` function of Pandas\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdsmzkZUwk_S"
      },
      "source": [
        "pivot = pd.pivot_table(\n",
        "    data = df,\n",
        "    index = 'CONTRIBUTING FACTOR VEHICLE 1',\n",
        "    columns = 'BOROUGH',\n",
        "    aggfunc = 'count',\n",
        "    values = 'COLLISION_ID'\n",
        ")\n",
        "pivot.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImuhF-Evwk_T"
      },
      "source": [
        "Find the dates with the most accidents. Can you figure out what happened on these days? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRjpidDSwk_T"
      },
      "source": [
        "df[\"CRASH DATE\"].value_counts().head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X-81qUGwk_W"
      },
      "source": [
        "Plot the number of accidents per day. (Hint: Ensure that your date column is in the right datatype and that it is properly sorted, before plotting)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQU3ghozwk_W"
      },
      "source": [
        "df[\"CRASH DATE\"] = pd.to_datetime(df[\"CRASH DATE\"], format=\"%m/%d/%Y\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "solution2": "hidden",
        "id": "1tb4v7QFwk_Y"
      },
      "source": [
        "(\n",
        "    df[\"CRASH DATE\"].value_counts() # count the number of accidents per day\n",
        "    .sort_index() # sort the dates\n",
        "    .resample('1M') # take periods of 1 month\n",
        "    .sum() # sum the number of accidents per month\n",
        "    .drop(pd.to_datetime('2019-04-30'), axis='index') # drop the current month\n",
        "    .plot() # plot the result\n",
        ");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y22n-5VJwk_Z"
      },
      "source": [
        "Plot the accidents in map. Use a scatter plot using the `plot(kind='scatter', x=..., y=....)` command, and use the `LATITUDE` and `LONGITUDE` parameters. (Hint: **You will have to remove bad data points before getting into the right visual result**. To do this, specify a selection condition to limit the lat/long values to be values that are proper for the NYC region.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8BgKbdQwk_a"
      },
      "source": [
        "cleandf = df[(df.LONGITUDE<-50) & (df.LONGITUDE>-74.5) & (df.LATITUDE< 41)]\n",
        "\n",
        "cleandf[ (df.LATITUDE>40) & (df.LATITUDE<41) & (df.LONGITUDE> -74.6) & (df.LONGITUDE<-50) ].plot(\n",
        "    figsize = (20,15),\n",
        "    kind = 'scatter',\n",
        "    x = 'LONGITUDE',\n",
        "    y = 'LATITUDE',\n",
        "    s = 1, # make each dot to be very small \n",
        "    alpha = 0.05 # makes each point 95% transparent\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBAhTwLrwk_h"
      },
      "source": [
        "Plot the accidents in map as above, but limit the data only to accidents with at least one injury."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "solution2": "hidden",
        "id": "FhpBeeY-wk_i"
      },
      "source": [
        "mask_injured = cleandf['NUMBER OF PERSONS INJURED']>=1\n",
        "injured_df = cleandf[mask_injured]\n",
        "\n",
        "injured_df.plot(\n",
        "    kind = 'scatter',\n",
        "    x = 'LONGITUDE',\n",
        "    y = 'LATITUDE',\n",
        "    s = 1, # make the size of the marker 1 pixel\n",
        "    figsize = (20,15), # increase the size of the figure\n",
        "    alpha = 0.05 # make each dot to be 95% transparent \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAOhFrcmwk_n"
      },
      "source": [
        "### 3.3 2d histograms, density plots, and contour plots\n",
        "In the picture above, we can visually see that Manhattan, especially eastern midtown, and the area downtown near the entrance to the bridges, has a higher density. We can also derive histograms and density plots on 2-dimensions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReZjGc3Gwk_n"
      },
      "source": [
        "#### 3.3.1 Hexagonal bin plot\n",
        "The hexbin plot created a 2-d histogram, where the color signals the number of points within a particular area. The `gridsize` parameter indicates the number of hexagones in the x direction. Higher values offer higher granularity, but very high values tend to create sparsity, when we do not have enough data points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6VvnKkhwk_n"
      },
      "source": [
        "# Hexbin plot\n",
        "cleandf.plot(\n",
        "    kind='hexbin',\n",
        "    x='LONGITUDE',\n",
        "    y='LATITUDE',\n",
        "    gridsize=100,\n",
        "    cmap=plt.cm.Blues,\n",
        "    figsize=(10, 7))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUFYZc6nwk_q"
      },
      "source": [
        "#### 3.3.2 2d density  and contour plots\n",
        "An alternative to the hexbin plots is to use density plots in two dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfq8xEMAwk_r"
      },
      "source": [
        "# Basic 2D density plot\n",
        "plt.subplots(figsize=(20, 15))\n",
        "\n",
        "# We take a sample, because density plots take a long time to compute\n",
        "# and a sample is typically as good as the full dataset\n",
        "sample = cleandf.sample(10000)\n",
        "\n",
        "sns.kdeplot(\n",
        "    sample.LONGITUDE,\n",
        "    sample.LATITUDE,\n",
        "    gridsize=100,  # controls the resolution\n",
        "    cmap=plt.cm.rainbow,  # color scheme\n",
        "    shade= True, # whether to have a density plot (True), or just the contours (False)\n",
        "    alpha=0.5,\n",
        "    shade_lowest=False,\n",
        "    n_levels=50  # How many contours/levels to have\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bArlRc1wk_v"
      },
      "source": [
        "# Basic 2D contour plot\n",
        "plt.subplots(figsize=(20, 15))\n",
        "\n",
        "# We take a sample, because density plots take a long time to compute\n",
        "# and a sample is typically as good as the full dataset\n",
        "sample = cleandf.sample(10000)\n",
        "\n",
        "sns.kdeplot(\n",
        "    sample.LONGITUDE,\n",
        "    sample.LATITUDE,\n",
        "    gridsize=100,\n",
        "    cmap=plt.cm.rainbow,\n",
        "    shade=False,\n",
        "    shade_lowest=False,\n",
        "    n_levels=25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww-z3flPwk_y"
      },
      "source": [
        "### 3.4 Combining plots\n",
        "So far, we examined how to create individual plots. We can even combine multiple plots together, using the ax parameter. So, let's say that we want to combine the scatter plots with the contour plot above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H81Nnlbhwk_y"
      },
      "source": [
        "sample = cleandf.sample(10000)\n",
        "\n",
        "scatterplot = cleandf.plot(\n",
        "    kind='scatter',\n",
        "    x='LONGITUDE',\n",
        "    y='LATITUDE',\n",
        "    figsize=(20, 15),\n",
        "    s=0.5,\n",
        "    alpha=0.1)\n",
        "\n",
        "sns.kdeplot(\n",
        "    sample.LONGITUDE,\n",
        "    sample.LATITUDE,\n",
        "    gridsize=100,\n",
        "    cmap=plt.cm.rainbow,\n",
        "    shade=False,\n",
        "    shade_lowest=False,\n",
        "    n_levels=20,\n",
        "    alpha=1,\n",
        "    ax=scatterplot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dt7a8sdwk_z"
      },
      "source": [
        "### 3.5 Bonus plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tt2wKdd_wk_z"
      },
      "source": [
        "# In the code below, we create three plots\n",
        "# One with all the accidents (in black)\n",
        "# One with all accidents that resulted in an injury (with orange)\n",
        "# One with all the accidents with deaths (with red dots) \n",
        "# Then wen combine the  plots using the \"ax\" parameter\n",
        "\n",
        "# We put conditions to keep only rows that \n",
        "# have reasonable values for LONGITUDE and LATITUDE\n",
        "# The & character is the \"AND\" in Pandas\n",
        "# the df.LATITUDE is equivalent to df['LATITUDE']\n",
        "cleandf = df[ (df.LATITUDE > 40) & (df.LATITUDE < 41) & (df.LONGITUDE < -72) & (df.LONGITUDE > -74.5) ]\n",
        "\n",
        "\n",
        "all_accidents_plot = cleandf.plot (\n",
        "    kind='scatter',\n",
        "    x = 'LONGITUDE',\n",
        "    y = 'LATITUDE',\n",
        "    figsize = (20, 18), # changes the size of the plot to be bigger and square\n",
        "    color = 'black',\n",
        "    alpha = 0.05, # makes the data points transparent (1 = opaque, 0 fully transparent)\n",
        "    s = 0.5 # make the size of the market just 0.5 pixel\n",
        ")\n",
        "\n",
        "# We will keep only entries with at least one injury\n",
        "# note that we cannot use the df.NUMBER OF PERSON INJURED notation\n",
        "# because the attribute contains spaces.\n",
        "mask_injured = cleandf['NUMBER OF PERSONS INJURED']>=1\n",
        "injured_plot = cleandf[mask_injured].plot (\n",
        "    kind='scatter',\n",
        "    x = 'LONGITUDE',\n",
        "    y = 'LATITUDE',\n",
        "    figsize = (20, 18), # changes the size of the plot to be bigger and square\n",
        "    alpha = 0.1, # makes the data points transparent (1 = opaque, 0 fully transparent)\n",
        "    color = 'orange',\n",
        "    ax = all_accidents_plot, # the ax parameter allows us to combine plots\n",
        "    s = 1 # make the size of the market just 1 pixel\n",
        ")\n",
        "\n",
        "# we want only accidents with at least one death\n",
        "mask_killed = cleandf['NUMBER OF PERSONS KILLED']>=1\n",
        "cleandf[mask_killed].plot (\n",
        "    kind='scatter',\n",
        "    x = 'LONGITUDE',\n",
        "    y = 'LATITUDE',\n",
        "    figsize = (20, 18), # changes the size of the plot to be bigger and square\n",
        "    alpha = 0.85, # makes the data points transparent (1 = opaque, 0 fully transparent)\n",
        "    s = 5, # make the size of the market 5 pixels, to be more visible\n",
        "    color = 'red',\n",
        "    ax = injured_plot # the ax parameter allows us to combine plots\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_HRc9-Vwk_0"
      },
      "source": [
        "# Same as above, but with a 2-d density estimation for the location of\n",
        "# accidents that resulted in at least one death\n",
        "\n",
        "cleandf = df[ (df.LATITUDE > 40) & (df.LATITUDE < 41) & (df.LONGITUDE < -72) & (df.LONGITUDE > -74.5) ]\n",
        "\n",
        "\n",
        "all_accidents_plot = cleandf.plot (\n",
        "    kind='scatter',\n",
        "    x = 'LONGITUDE',\n",
        "    y = 'LATITUDE',\n",
        "    figsize = (20, 18), # changes the size of the plot to be bigger and square\n",
        "    color = 'black',\n",
        "    alpha = 0.05, # makes the data points transparent (1 = opaque, 0 fully transparent)\n",
        "    s = 0.5 # make the size of the market just 0.5 pixel\n",
        ")\n",
        "\n",
        "# We will keep only entries with at least one injury\n",
        "# note that we cannot use the df.NUMBER OF PERSON INJURED notation\n",
        "# because the attribute contains spaces.\n",
        "mask_injured = cleandf['NUMBER OF PERSONS INJURED']>=1\n",
        "injured_plot = cleandf[mask_injured].plot (\n",
        "    kind='scatter',\n",
        "    x = 'LONGITUDE',\n",
        "    y = 'LATITUDE',\n",
        "    figsize = (20, 18), # changes the size of the plot to be bigger and square\n",
        "    alpha = 0.1, # makes the data points transparent (1 = opaque, 0 fully transparent)\n",
        "    color = 'orange',\n",
        "    ax = all_accidents_plot, # the ax parameter allows us to combine plots\n",
        "    s = 1 # make the size of the market just 1 pixel\n",
        ")\n",
        "\n",
        "# we want only accidents with at least one death\n",
        "mask_killed = cleandf['NUMBER OF PERSONS KILLED']>=1\n",
        "killed_plot = cleandf[mask_killed].plot (\n",
        "    kind='scatter',\n",
        "    x = 'LONGITUDE',\n",
        "    y = 'LATITUDE',\n",
        "    figsize = (20, 18), # changes the size of the plot to be bigger and square\n",
        "    alpha = 0.85, # makes the data points transparent (1 = opaque, 0 fully transparent)\n",
        "    s = 5, # make the size of the market 5 pixels, to be more visible\n",
        "    color = 'red',\n",
        "    ax = injured_plot # the ax parameter allows us to combine plots\n",
        ")\n",
        "\n",
        "sns.set_style(\"white\")\n",
        "mask = cleandf['NUMBER OF PERSONS KILLED']>=1\n",
        "sample = cleandf[mask] # .sample(10000)\n",
        "\n",
        "sns.kdeplot(sample.LONGITUDE, sample.LATITUDE, gridsize=100,\n",
        "            cmap=plt.cm.BuGn, shade=False, shade_lowest=True, n_levels=20, alpha=0.75, ax = killed_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx7eMUsx6AuY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}