{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Text_Analytics_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataMining_and_MachineLearning/blob/master/week9/Text_Analytics_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onO46LysT2dh"
      },
      "source": [
        "# Data Mining and Machine Learning - Week 7\n",
        "# Text Analytics 2\n",
        "\n",
        "[Text Analytics](https://people.ischool.berkeley.edu/~hearst/text-mining.html) (or text mining) is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\" Written resources may include websites, books, emails, reviews, and articles.\n",
        "\n",
        "### This will be one of the most difficult lab sessions of the semester. Don't hesitate to ask if anything is unclear!\n",
        "\n",
        "### Table of Contents\n",
        "#### 0. Project: Git and GitHub\n",
        "#### 1. Recap on text representation\n",
        "* 1.1 Some important concepts\n",
        "* 1.2 Bag of Words (BOW)\n",
        "* 1.3 TF-IDF\n",
        "\n",
        "#### 2. Introduction to Gensim and Word Embedding\n",
        "* 2.1 Word embedding with Word2Vec\n",
        "* 2.2 Exercise\n",
        "\n",
        "#### 3. Complaints Classification: TF-IDF vs. Doc2Vec\n",
        "* 3.1 Load and clean data\n",
        "* 3.2 EDA\n",
        "* 3.3 Classification using TF-IDF and Logistic Regression\n",
        "* 3.4 Classification using Doc2Vec and Logistic Regression\n",
        "\n",
        "Author: Luc Kunz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcoTbUAtziKl"
      },
      "source": [
        "## 0. Project: Git and GitHub\n",
        "For the project, you will have to work with Git and GitHub. The following documentation can be useful to you:\n",
        "* [Git and GitHub tutorial for beginners](https://www.youtube.com/playlist?list=PL4cUxeGkcC9goXbgTDQ0n_4TBzOO0ocPR)\n",
        "* [GitHub Desktop video 1](https://www.youtube.com/watch?v=fJtyf62yAb8)\n",
        "* [GitHub Desktop video 2](https://www.youtube.com/watch?v=GqNAD4XoZ6k)\n",
        "* [Git Cheat Sheet](https://education.github.com/git-cheat-sheet-education.pdf)\n",
        "\n",
        "If you're having troubles completing your project using Git and/or GitHub Desktop, please let me know by email/slack and we can arrange an additional lab session on how to do a python project with Git and GitHub Desktop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6OazeNbVotv"
      },
      "source": [
        "# Import required packages\n",
        "import gensim\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import spacy\n",
        "import string\n",
        "import math\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load English language model of spacy\n",
        "sp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0VsVUPLT4Tn"
      },
      "source": [
        "## 1. Recap on text representation\n",
        "In order to be able to use texts as inputs for classification, we have to transform them into numbers (i.e. vectors). There are several ways of doing this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q95eek-OIIPq"
      },
      "source": [
        "### 1.1 Some important concepts\n",
        "* Document = some text i.e. a string (e.g. a sentence, a tweet, paragraph of text, book, news article, etc.).\n",
        "* Corpus = collection of documents.\n",
        "* Dictionary = list of unique tokens in (preprocessed) corpus.\n",
        "* Vector = mathematical representatation of a document (e.g. Bag of Words).\n",
        "* Model = algorithm used for transforming vectors from one representation to another (e.g. TF-IDF)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ0TKUegLawc"
      },
      "source": [
        "# A document\n",
        "doc = 'Tom confessed that he had fallen in love with me' # single quotes\n",
        "doc = \"Tom confessed that he had fallen in love with me\" # double quotes\n",
        "doc = \"\"\"Tom confessed that he had fallen in love with me.\"\"\" # triple quotes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6jZsQ-LLbIG"
      },
      "source": [
        "# A corpus\n",
        "d1 = \"Tom confessed that he had fallen in love with me\"\n",
        "d2 = \"We musn't joke around with love\"\n",
        "d3 = \"Human-caused climate change has caused land ice to melt and ocean water to expand\"\n",
        "d4 = \"Climate change is not really happening\"\n",
        "d5 = \"We asked Tom what he wanted for Christmas\"\n",
        "corpus = [d1, d2, d3, d4, d5]\n",
        "\n",
        "# Preprocessing\n",
        "from gensim.utils import simple_preprocess\n",
        "processed_corpus = []\n",
        "for doc in corpus:\n",
        "  processed_corpus.append(simple_preprocess(doc))\n",
        "processed_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHO4IVeaN03k"
      },
      "source": [
        "# A dictionary\n",
        "from gensim import corpora\n",
        "dictionary = corpora.Dictionary(processed_corpus)\n",
        "print(dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3v9-RrzN0rd"
      },
      "source": [
        "dictionary.token2id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHvrJj5yT8uQ"
      },
      "source": [
        "### 1.2 Bag of Words (BOW)\n",
        "\n",
        "Bag of Words is the simplest approach to achieve the transformation of documents into vectors. It is divided into two basic steps:\n",
        "* Create a dictionary of unique words from the corpus.\n",
        "* Analyse the documents, i.e. for each word in the dictionary and each document, add 1 if the word is in the document, otherwise 0.\n",
        "\n",
        "Let's try to code it from scratch using spacy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms5chNYf5lPt"
      },
      "source": [
        "# Tokens in document\n",
        "def get_tokens(document):\n",
        "  doc_tokens = []\n",
        "  for token in sp(document):\n",
        "      if (token.is_punct == False) and (token.is_space == False):\n",
        "        doc_tokens.append(token.lower_)\n",
        "  return doc_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkeTK3afT0yz"
      },
      "source": [
        "# List of unique words in corpus (dictionary)\n",
        "def vocabulary(corpus):\n",
        "  # Delare output\n",
        "  word_list = []\n",
        "  # Loop documents - lower each word and add it to the output\n",
        "  for document in corpus:\n",
        "    spacy_doc = sp(document)\n",
        "    for token in spacy_doc:\n",
        "      if token.lower_ not in word_list and (token.is_punct == False) and (token.is_space == False):\n",
        "        word_list.append(token.lower_)\n",
        "  # Return output\n",
        "  return word_list\n",
        "    \n",
        "vocabulary(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFNFfyN6UkAh"
      },
      "source": [
        "We now have a function to get the words of a document and a function to get the unique words of a corpus of documents. We can use them to create the Bag of Words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1DSadtqT1ri"
      },
      "source": [
        "# Bag of Words\n",
        "def bow(document, corpus):\n",
        "  # Get tokens\n",
        "  doc_tokens = get_tokens(document)\n",
        "  corpus_tokens = vocabulary(corpus)\n",
        "  # Initialization\n",
        "  bag = {}\n",
        "  for token in corpus_tokens:\n",
        "    bag[token] = 0\n",
        "  # Add 1 if token is in document\n",
        "  for token in doc_tokens:\n",
        "    bag[token] += 1\n",
        "  # Return\n",
        "  return bag\n",
        "\n",
        "bow(d1, corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbfVjwmYYU0H"
      },
      "source": [
        "# Dataframe - all documents in corpus\n",
        "bag_of_words = []\n",
        "for doc in corpus:\n",
        "  bag = bow(doc, corpus)\n",
        "  bag_of_words.append(bag)\n",
        "  \n",
        "pd.DataFrame(bag_of_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr1EQmIeYSqi"
      },
      "source": [
        "Remarks:\n",
        "* This is not perfect (e.g. we could remove stopwords, use n-grams, lemmas).\n",
        "* We can use [CountVectorizer](https://https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from sklearn as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTCKo-uohh_X"
      },
      "source": [
        "# Using CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "bag_of_words = vectorizer.fit_transform(corpus).todense()\n",
        "bag_of_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pG0HqjabiUam"
      },
      "source": [
        "# Features\n",
        "vectorizer.vocabulary_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYdVJrtTiSbZ"
      },
      "source": [
        "# DataFrame\n",
        "bag_of_words = pd.DataFrame(bag_of_words, columns=vectorizer.get_feature_names())\n",
        "bag_of_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoU9xUZLkAZu"
      },
      "source": [
        "Advantages of BOW:\n",
        "* No need of huge corpus of words to get good results in practice.\n",
        "* Easy to understand (i.e. not mathematically complex).\n",
        "\n",
        "Disadvantages of BOW:\n",
        "* A lot of zeros (imagine a corpus of 1000 articles) --> consume memory and space.\n",
        "* Does not maintain any context information (\"I eat a fish\" vs. \"A fish eats me\").\n",
        "* Half solutions: n-grams, specifiying min_df and max_df (see [documentation](https://https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CkpoOenUv7J"
      },
      "source": [
        "### 1.3 TF-IDF\n",
        "TF-IDF is a type of bag of words approach where instead of adding zeros and ones in the embedding vector, you add floating numbers that contain more useful information compared to zeros and ones. The idea is to emphasize words that appear in few documents in the corpus. A word that appear many times but only in one document will have a high value (close to one) compared to words that appear many times in many documents. This word is then very useful to identify the document.\n",
        "\n",
        "TF(word, document) = Term frequency = (Number of occurences of a word in document)/(Total words in the document)\n",
        "- greater if word appears many times in document\n",
        "\n",
        "IDF(word) = Inverse Document Frequency = Log((Total number of documents)/(Number of documents containing the word))\n",
        "- greater if word appears in fewer doucuments\n",
        "\n",
        "TF-IDF = TF*IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsTBjUmWUywM"
      },
      "source": [
        "# Term frequency (TF)\n",
        "def tf(document):\n",
        "  # Get tokens\n",
        "  tokens = get_tokens(document)\n",
        "  # Initialization\n",
        "  term_freq = {}\n",
        "  for token in tokens:\n",
        "    term_freq[token] = 0\n",
        "  # Increment\n",
        "  for token in tokens:\n",
        "    term_freq[token] += 1/len(tokens)\n",
        "  # Return\n",
        "  return term_freq\n",
        "\n",
        "tf(d3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3h8kaUxrmn5"
      },
      "source": [
        "# Inverse document frequency\n",
        "def idf(corpus):\n",
        "  # Get list of unique words in corpus\n",
        "  voc = vocabulary(corpus)\n",
        "  # Initialization\n",
        "  inv_doc_freq = {}\n",
        "  for word in voc:\n",
        "    inv_doc_freq[word] = 0\n",
        "  # Number of apparition of word\n",
        "  for word in voc:\n",
        "    for document in corpus:\n",
        "      doc_tokens = get_tokens(document)\n",
        "      if word in doc_tokens:\n",
        "        inv_doc_freq[word] += 1\n",
        "  #print(inv_doc_freq)\n",
        "  #print(\"\\n----------------------\\n\")\n",
        "  # IDF\n",
        "  inv_doc_freq = {k: math.log(len(corpus) / inv_doc_freq[k]) for k in inv_doc_freq.keys()}\n",
        "  # Return\n",
        "  return inv_doc_freq\n",
        "\n",
        "idf(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfgMl15xwOeP"
      },
      "source": [
        "# TF-IDF\n",
        "def tfidf(document, corpus):\n",
        "  # TF\n",
        "  tf_bag = tf(document)\n",
        "  # IDF\n",
        "  idf_bag = idf(corpus)\n",
        "  # TF*IDF\n",
        "  tfidf_bag = {k: tf_bag[k]*idf_bag[k] for k in tf_bag.keys()}\n",
        "  \n",
        "  return tfidf_bag\n",
        "\n",
        "tfidf(d3, corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tIVDQRtxhL0"
      },
      "source": [
        "# DataFrame\n",
        "bag_of_words_tfidf = []\n",
        "for doc in corpus:\n",
        "  bag = tfidf(doc, corpus)\n",
        "  bag_of_words_tfidf.append(bag)\n",
        "  \n",
        "pd.DataFrame(bag_of_words_tfidf).fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oj3x4fCLyHcA"
      },
      "source": [
        "Remarks:\n",
        "* This is not perfect (e.g. we could remove stopwords, use n-gramsm lemmas)\n",
        "* We can use [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) from sklearn as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZGWejXoyG3r"
      },
      "source": [
        "# Using TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "bag_of_words = vectorizer.fit_transform(corpus).todense()\n",
        "\n",
        "# DataFrame\n",
        "bag_of_words = pd.DataFrame(bag_of_words, columns=vectorizer.get_feature_names())\n",
        "bag_of_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFsl7S2FyxxH"
      },
      "source": [
        "Advantage of TF-IDF:\n",
        "* Smart way of representing documents in corpus. More information is provided.\n",
        "\n",
        "Disadvantages of TF-IDF (same as for BOW):\n",
        "* A lot of zeros (imagine a corpus of 1000 articles) --> consume memory and space\n",
        "* Does not maintain any context information (\"I eat a fish\" vs. \"A fish eats me\")\n",
        "* Half solutions: n-grams, specifiying min_df and max_df (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkm6F65kUzR_"
      },
      "source": [
        "## 2. Introduction to Gensim and Word Embedding\n",
        "\n",
        "In the following, we illustrate how we can find out the relations between words in a dataset, compute the similarity between them, or use the vector representation of those words as input for other applications such as text classification or clustering.\n",
        "\n",
        "We use Gensim. A complete tutorial can be found [here](https://www.tutorialspoint.com/gensim/gensim_introduction.htm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-lFcpJJPXAq"
      },
      "source": [
        "### 2.1 Word Embedding with Word2Vec\n",
        "\n",
        "Word embediing approaches use deep learning and neural network-based techniques to convert words into corresponding vectors so that semantically similar vectors are close to each other in an N-dimensional space, where N refers to the dimensions of the vectors. The underlying assumption is that two words sharing similar contexts also share a similar meaning and consequently a similar vector representation from the model.\n",
        "\n",
        "Two word embedding methods:\n",
        "* Word2Vec by Google\n",
        "* GloVe (Global vectors for Word Representation) by Stanford\n",
        "\n",
        "Word2Vec gives astonishing results. Its ability to maintain a semantic relationship is reflected in a classic example where if you have a vector for the word \"King\" and you remove the vector represented by the word \"Man\" from the \"King\" and add \"Woman\", you get a vector that is close to the vector \"Queen\". \n",
        "\n",
        "* King - Man + Woman = Queen\n",
        "\n",
        "Second example: \"dog\", \"puppy\" and \"pup\" are often used in similar situations, with similar surrounding words like \"good\", \"fluffy\" or \"cute\", and according to Word2Vec they will therefore share a similar vector representation.\n",
        "\n",
        "In real applications, Word2Vec models are created from billions of documents. For example, [Google's Word2Vec model](https://code.google.com/archive/p/word2vec/) is formed from 3 million words and phrases.\n",
        "\n",
        "GloVe is an extension of Word2Vec. More information [here](https://nlp.stanford.edu/projects/glove/). \n",
        "\n",
        "More detail on word embedding will be given in the class following this lab session. You can also click [here](https://www.youtube.com/watch?v=yFFp9RYpOb0) to watch a video on Word2Vec."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ONCENA5RHYX"
      },
      "source": [
        "# Get texts from Wikipedia\n",
        "def get_text(url):\n",
        "  scrapped_data = urllib.request.urlopen(url)\n",
        "  article = scrapped_data.read()\n",
        "  parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "  paragraphs = parsed_article.find_all('p')\n",
        "  article_text = \"\"\n",
        "  for p in paragraphs:\n",
        "    article_text += p.text\n",
        "  return article_text\n",
        "\n",
        "machine_learning = get_text(\"https://en.wikipedia.org/wiki/Machine_learning\")\n",
        "ai = get_text(\"https://en.wikipedia.org/wiki/Artificial_intelligence\")\n",
        "\n",
        "machine_learning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrTQGSZWZlBx"
      },
      "source": [
        "ai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfGGJFCLaF_F"
      },
      "source": [
        "# Group texts in list\n",
        "texts = [machine_learning, ai]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56Z0fduhRMb5"
      },
      "source": [
        "# Create tokenizer function for preprocessing\n",
        "def spacy_tokenizer(text):\n",
        "\n",
        "    # Define stopwords, punctuation, and numbers\n",
        "    stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "    punctuations = string.punctuation\n",
        "    numbers = \"0123456789\"\n",
        "\n",
        "    # Create spacy object\n",
        "    mytokens = sp(text)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # Remove sufix like \".[1\" in \"experience.[1\"\n",
        "    mytokens_2 = []\n",
        "    for word in mytokens:\n",
        "      for char in word:\n",
        "        if (char in punctuations) or (char in numbers):\n",
        "          word = word.replace(char, \"\")\n",
        "      if word != \"\":\n",
        "        mytokens_2.append(word)\n",
        "\n",
        "    # Return preprocessed list of tokens\n",
        "    return mytokens_2\n",
        "\n",
        "# Tokenize texts\n",
        "processed_texts = []\n",
        "for text in texts:\n",
        "  processed_text = spacy_tokenizer(text)\n",
        "  processed_texts.append(processed_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uJoiMgsabwj"
      },
      "source": [
        "for processed_text in processed_texts:\n",
        "  print(processed_text[:20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYbGv4LbRRlZ"
      },
      "source": [
        "# Word embedding \n",
        "### Parameters: \n",
        "#     - min_count: minimum number of occurence of single word in corpus to be taken into account\n",
        "#     - size: dimension of the vectors representing the tokens\n",
        "#     - IMPORTANT: processed_texts must be a list of lists of tokens object!\n",
        "word2vec = Word2Vec(processed_texts, min_count=2, size=100)\n",
        "vocab = word2vec.wv.vocab\n",
        "print(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynLxnGryTXKc"
      },
      "source": [
        "# Vector\n",
        "v1 = word2vec.wv['intelligence'] \n",
        "v1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCQRhSF5bPFI"
      },
      "source": [
        "# Similar vectors/words\n",
        "sim_words = word2vec.wv.most_similar('intelligence')\n",
        "sim_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRBwWMjtcrOH"
      },
      "source": [
        "# Similarity between two words\n",
        "word2vec.wv.similarity('computer', 'animal')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2Qsloo6dz3i"
      },
      "source": [
        "word2vec.wv.similarity('computer', 'machine')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwoQg0xEf9kN"
      },
      "source": [
        "Remarks:\n",
        "* Many things can be done with Gensim (e.g. [topic modelling](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/))\n",
        "* There exists also `Doc2Vec`, which is used to create a vectorised representation of a group of words (i.e. a document) taken collectively as a single unit (illustrated in the next section)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-_83p20NjO-"
      },
      "source": [
        "### 2.2 Exercise\n",
        "Analyze the wikipedia article on [Coronavirus](https://en.wikipedia.org/wiki/Coronavirus) as above. Follow the steps and send your answers and code @Luc Kunz on Slack (direct message) or via Zoom (private). This is a good way to improve your participation grade."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTiVRKpjcrJT"
      },
      "source": [
        "# 1. Get text from URL - use the get_text() function defined above\n",
        "coronavirus = get_text('https://en.wikipedia.org/wiki/Coronavirus')\n",
        "\n",
        "# 2. Processing - tokenization using the spacy_tokenizer() function\n",
        "processed_corona = spacy_tokenizer(coronavirus)\n",
        "processed_corona[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz5BBzckeLAe"
      },
      "source": [
        "# 3. What is the number of occurence of the word \"virus\"?\n",
        "count = 0\n",
        "for word in processed_corona:\n",
        "  if word == 'virus':\n",
        "    count += 1\n",
        "count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-1RbfX7fpzO"
      },
      "source": [
        "# 4. Create a Word2Vec representation of the article with a min_count of 1 and a vector size of 50\n",
        "word2vec_corona = Word2Vec([processed_corona], min_count=1, size=50)\n",
        "\n",
        "# 5. What is the 10 most similar words of \"virus\"\n",
        "word2vec_corona.wv.most_similar('virus')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3he4niYadil"
      },
      "source": [
        "## 3. Complaints Classification: TF-IDF vs. Doc2Vec\n",
        "We classify consumer finance complaints into 12 pre-defined categories using:\n",
        "* TF-IDF and logistic regression\n",
        "* Doc2Vec and logistic regression\n",
        "\n",
        "We use the same tokenizer function, train-test split, classification algorithm, etc. The only difference is the mathematical representation (i.e. the vectorization from the tokens) of the complaints:\n",
        "* TF-IDF: important words (or n-grams) are words (n-grams) that frequently appear in few documents.\n",
        "* Doc2Vec: similar documents must be close to each other in n-dimensional space. Focus on the context of the documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz_p-bl8TFQb"
      },
      "source": [
        "### 3.1 Load and clean data\n",
        "We work with a sample of a large data set from Data.gov that can be found on [here](https://catalog.data.gov/dataset/consumer-complaint-database).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiRTQshsihUu"
      },
      "source": [
        "# Load data from GitHub\n",
        "path = \"https://raw.githubusercontent.com/michalis0/DataMining_and_MachineLearning/master/week9/data/complaints_sample.csv\"\n",
        "df = pd.read_csv(path, index_col=0)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cys9L2XidZsq"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyg-rztyPJ6e"
      },
      "source": [
        "The data set includes 18 columns and 9101 rows describing consumer complaints about financial products. In this case, we want to predict the `Product` categorie based on the text of the complaint (i.e. `Consumer complaint narrative`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mgx17mnpy2s"
      },
      "source": [
        "# Select columns of interest\n",
        "data = df[[\"Product\", \"Consumer complaint narrative\"]]\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhB01GG6QXVP"
      },
      "source": [
        "Around 2/3 of the complaints are null values. They are not useful for the prediction so we drop them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbno4wghqCNH"
      },
      "source": [
        "# Drop NaN\n",
        "print(data.isnull().sum())\n",
        "data = data.dropna().reset_index(drop=True)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgb3yc3KqCLr"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCasthTDQrYb"
      },
      "source": [
        "We end up with 3137 complaints for which we would like to predict the product concerned."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc8z_6SkTIx2"
      },
      "source": [
        "### 3.2 EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JQ8GFf0ihb3"
      },
      "source": [
        "# Total number of words - over 600,000\n",
        "data['Consumer complaint narrative'].apply(lambda x: len(x.split(' '))).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppQTFaZ6t9dy"
      },
      "source": [
        "# Sample\n",
        "data['Consumer complaint narrative'].sample().values[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDpSTJyZQ6O5"
      },
      "source": [
        "The data has been anonymized (i.e. names, dates, IDs, etc. have been replaced by XXXX)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdDpzPPouaSN"
      },
      "source": [
        "# Imbalanced dataset\n",
        "data.Product.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDon6qpmRMzo"
      },
      "source": [
        "There are 17 categories. We group some of them together (e.g. `Credit card`, `Prepaid card`, and `Credit or prepaid card`) because they are sub-categories of each other. We end up with 12 categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er1kzzv3vBSt"
      },
      "source": [
        "# Clean\n",
        "dic_replace = {'Credit reporting':'Credit reporting, credit repair services, or other personal consumer reports', \n",
        "               'Credit card':'Credit card or prepaid card', \n",
        "               'Payday loan':'Payday loan, title loan, or personal loan', \n",
        "               'Money transfers':'Money transfer, virtual currency, or money service',\n",
        "               'Prepaid card':'Credit card or prepaid card',\n",
        "               'Virtual currency':'Money transfer, virtual currency, or money service'}\n",
        "data.replace(dic_replace, inplace=True)\n",
        "data.Product.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEeOpaMQtgAA"
      },
      "source": [
        "# Plot number of complaints per category\n",
        "cnt_pro = data['Product'].value_counts()\n",
        "plt.figure(figsize=(12,4))\n",
        "sns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\n",
        "plt.ylabel('Number of Occurrences', fontsize=12)\n",
        "plt.xlabel('Product', fontsize=12)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JdF1razudYJ"
      },
      "source": [
        "# Base rate\n",
        "round(len(data[data.Product == \"Credit reporting, credit repair services, or other personal consumer reports\"]) / len (data), 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBr6OSytTLgD"
      },
      "source": [
        "### 3.3 Classification using TF-IDF and Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eU2xSf8sTevp"
      },
      "source": [
        "# Import packages\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXHnUbjerLPD"
      },
      "source": [
        "# Define tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "\n",
        "    punctuations = string.punctuation\n",
        "    stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "    # Create token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = sp(sentence)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # Remove anonymous dates and people\n",
        "    mytokens = [ word.replace('xx/', '').replace('xxxx/', '').replace('xx', '') for word in mytokens ]\n",
        "    mytokens = [ word for word in mytokens if word not in [\"xxxx\", \"xx\", \"\"] ]\n",
        "\n",
        "    # Return preprocessed list of tokens\n",
        "    return mytokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38ttjGhrihZF"
      },
      "source": [
        "# Select features\n",
        "X = data['Consumer complaint narrative'] # the features we want to analyze\n",
        "ylabels = data['Product'] # the labels, or answers, we want to test against\n",
        "\n",
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=1234)\n",
        "\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOHhL3ZgxUZh"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tTbx3oswIan"
      },
      "source": [
        "%%time\n",
        "# Define vectorizer\n",
        "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), tokenizer=spacy_tokenizer)\n",
        "\n",
        "# Define classifier\n",
        "classifier = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC3Pbqxlxod2"
      },
      "source": [
        "# Predictions\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "print(round(accuracy_score(y_test, y_pred), 4))\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "fig, ax = plt.subplots(figsize=(15,15))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Thl75Ei5TRZz"
      },
      "source": [
        "### 3.4 Classification using Doc2Vec and Logistic Regression\n",
        "We now try to do the same exercise, but using [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDBzqEryVgSY"
      },
      "source": [
        "# Tokenize data - same tokenizer function as before\n",
        "%%time\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "sample_tagged = data.apply(lambda r: TaggedDocument(words=spacy_tokenizer(r['Consumer complaint narrative']), tags=[r.Product]), axis=1)\n",
        "print(sample_tagged.head(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hatOMpSPAqw"
      },
      "source": [
        "sample_tagged.values[10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdSvlQf26uZb"
      },
      "source": [
        "# Train test split - same split as before\n",
        "train_tagged, test_tagged = train_test_split(sample_tagged, test_size=0.2, random_state=1234)\n",
        "\n",
        "train_tagged"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNGTNrwo7mhN"
      },
      "source": [
        "test_tagged"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY_KwtL8ThRV"
      },
      "source": [
        "# Allows to speed up a bit\n",
        "import multiprocessing\n",
        "cores = multiprocessing.cpu_count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhkoBMP_o0HN"
      },
      "source": [
        "# Define Doc2Vec and build vocabulary\n",
        "from gensim.models import Doc2Vec\n",
        "\n",
        "model_dbow = Doc2Vec(dm=0, vector_size=30, negative=6, hs=0, min_count=1, sample=0, workers=cores, epoch=300)\n",
        "model_dbow.build_vocab([x for x in train_tagged.values])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqBwLmHGU8XD"
      },
      "source": [
        "We now train the distributed bag of words model. In short, it trains a neural network and the optimal weights are the coefficients of the vectors of the documents. Therefore, similar documents will be close to each other in the N-dimentional space (N being the size of the vectors). More information on this [here](https://thinkinfi.com/simple-doc2vec-explained/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDU2E5fCpHVj"
      },
      "source": [
        "# Train distributed Bag of Word model\n",
        "model_dbow.train(train_tagged, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hq_UN6_U0hF"
      },
      "source": [
        "# Select X and y\n",
        "def vec_for_learning(model, tagged_docs):\n",
        "    sents = tagged_docs.values\n",
        "    targets, regressors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=100)) for doc in sents])\n",
        "    return targets, regressors\n",
        "\n",
        "y_train, X_train = vec_for_learning(model_dbow, train_tagged)\n",
        "y_test, X_test = vec_for_learning(model_dbow, test_tagged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O98HjFkWs6QI"
      },
      "source": [
        "# Each document (i.e. complaint) is now a vector in the space of 30 dimentions.\n",
        "# Similar complaints should have similar vector representation.\n",
        "X_train[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0fd2XSFVz1r"
      },
      "source": [
        "# Fit model on training set - same algorithm as before\n",
        "logreg = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = logreg.predict(X_test)\n",
        "\n",
        "# Evaluate model\n",
        "print(round(accuracy_score(y_test, y_pred), 4))\n",
        "conf_mat = confusion_matrix(y_test, y_pred)\n",
        "fig, ax = plt.subplots(figsize=(15,15))\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_jHZwiHjnu_"
      },
      "source": [
        "## References\n",
        "* https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4\n",
        "* https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f"
      ]
    }
  ]
}